# Reproducibility Artifacts for "Integrated Reproducibility with Self-describing Machine Learning Models"

The paper presents MERIT, a reproducibility system fully integrated into the Tribuo ML library. To use the most up-to-date version of MERIT yourself, you can add Tribuo to your Java projects using Maven, Gradle, or from source - [installation and documentation can be found here](https://tribuo.org). Otherwise, this document explains how to execute our evaluation with the version of MERIT we used. 

Our evaluation uses Docker and consists of three Jupyter notebooks that each train and then reproduce a different set of models. In total, they will train and then reproduce 48 models, and save the results to a csv file.

The results from when we originally ran this evaluation are stored in the top-level directory titled `author-results`, with the models we trained stored in the top-level directory `author-models`.

## Table of Contents
1. [Intro](#reproducibility-artifacts-for-integrated-reproducibility-with-self-describing-machine-learning-models)
2. [Table of Contents](#table-of-contents)
3. [Estimated Time and Resources](#estimated-time-and-resources)
4. [Reproducing the results](#reproducing-the-results)
5. [Building the Docker image (if pulling the image doesn't work)](#building-the-docker-image-if-pulling-the-image-doesnt-work)
6. [Using MERIT and Tribuo on your own](#using-merit-and-tribuo-on-your-own)
7. [Explanation of directory structure](#directory-structure)


## Estimated Time and Resources

These instructions have been tested on a machine with 32GB RAM and an Intel Xeon E-2276M CPU @ 2.80GHz
At a minimum, you need to be able to provide the JVM with ~4GB of heap space. 

We reproduced this work using the following instructions on a fresh machine in ~10 minutes. Included within the time was downloading the image, running the experiments, and checking the results. 

# Reproducing the results

The simplest way to reproduce the results is to pull the pre-built Docker image from Docker Hub. This image is almost 4GB so it might take a few minutes to download. To pull the image: use a computer [with Docker installed](https://docs.docker.com/get-docker/), open a command-line prompt and run the following command. 
```
docker pull jwonsil/merit-artifact
```
(If you'd rather build the image yourself, check the next section.)

Once you have pulled this image, start the local Jupyter Lab server using the following command.
```
docker run --rm -p 8888:8888 jwonsil/merit-artifact
```
It will take a few seconds to start up, once it has there will be instructions on how to access it. 
```
To access the server, open this file in a browser:
        file:///home/jupyter/.local/share/jupyter/runtime/jpserver-1-open.html
    Or copy and paste one of these URLs:
        http://1a6bda217368:8888/lab?token=XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
     or http://127.0.0.1:8888/lab?token=XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
```
Ctrl+Click the bottom-most link that starts with `http://127.0.0.1:8888` to be taken to the Jupyter Lab instance. 

Navigate to the `eval` folder and there will be 3 notebooks along with some folders. Running each notebook from top to bottom in a linear fashion will train and then reproduce all 48 models. Some of these models might take a few minutes to train and re-train. 

Once all the notebooks have completed, open the `results` directory to see a csv file for each notebook. Within these files are columns named `Equivalent Evaluation` and `Model Prov Diff`. The models were all successfully reproduced if  `Equivalent Evaluation` is all true and `Model Prov Diff` only contains timestamps. 

Closing the terminal with the server running, or pressing ctrl+c within in will stop the server. 

## Building the Docker image (if pulling the image doesn't work)

You should not need to build the image yourself, but in case you want to try here are the instructions. Pulling the pre-built image is likely faster, and more guaranteed to work in the off chance one of the sites hosting a dependency linked in the Dockerfiles no longer works.

These steps are for linux machines with Docker installed, they build the image and will likely take a while. 
```
cd tribuo-env
docker build . -t tribuo-env
cd ..

docker build . -t tribuo-notebook
```

To run the image as a container:
```
docker run --rm -p 8888:8888 tribuo-notebook
```
Then browse to localhost:8888 in your browser. Ctrl+c in the terminal where you ran the container kills the notebook server. 

# Using MERIT and Tribuo on your own
Tribuo and MERIT can be easily included into Java projects through Maven, Gradle, or linking to prebuilt jars. Visit [tribuo.org](https://tribuo.org) to learn how. 

# Directory Structure

```
ðŸ“‚ Repository Root
â”œâ”€â”€ ðŸ“‚author-models # All the models we trained during our evaluation
â”‚   â”œâ”€â”€ ðŸ“‚classification 
â”‚   â”‚   â”œâ”€â”€ ðŸ“œ3-nn.model
â”‚   â”‚   ...
â”‚   â”‚   â””â”€â”€ ðŸ“œrf.model
â”‚   â””â”€â”€ ðŸ“‚regression
â”‚       â”œâ”€â”€ ðŸ“œ3-nn.model
â”‚       ...
â”‚       â””â”€â”€ ðŸ“œrf-reg.model
â”œâ”€â”€ ðŸ“‚author-results # Our results from running these experiments
â”‚   â”œâ”€â”€ ðŸ“œconfigResults.csv
â”‚   â”œâ”€â”€ ðŸ“œmultilabelResults.csv
â”‚   â””â”€â”€ ðŸ“œresults.csv
â”œâ”€â”€ ðŸ“‚eval # The main directory for our evaluation, contains the notebooks, data, intermediate files, and results.
â”‚   â”œâ”€â”€ ðŸ“‚configs
â”‚   â”‚   â”œâ”€â”€ ðŸ“œall-classification-config.xml
â”‚   â”‚   ...
â”‚   â”‚   â””â”€â”€ ðŸ“œmnist-config.xml
â”‚   â”œâ”€â”€ ðŸ“‚data # This is populated once the container is built
â”‚   â”œâ”€â”€ ðŸ“‚models # Trained models are saved here
â”‚   â”œâ”€â”€ ðŸ“‚results # The results of the experiments are saved here
â”‚   â”œâ”€â”€ ðŸ“œreproduce-models.ipynb # Experimental script
â”‚   â”œâ”€â”€ ðŸ“œreproduce-multilabel-config.ipynb # Experimental script
â”‚   â””â”€â”€ ðŸ“œreproduce-from-configs.ipynb # Experimental script
â”œâ”€â”€ ðŸ“‚example-provenance # Contains example provenance objects that are too big to put in the paper. Each one is about ~2000 lines of "pretty printed" provenance 
â”‚   â”œâ”€â”€ ðŸ“œExtraTreesModelProvenance.txt
â”‚   â””â”€â”€ ðŸ“œRandomForestModelProvenance.txt
â”œâ”€â”€ ðŸ“‚ reproduce-serialized # This directory contains a Java program for reproducing serialized models for the cross-architecture eval in the from of a unit test. 
â”‚   â”œâ”€â”€ ðŸ“œpom.xml
â”‚   â””â”€â”€ ðŸ“‚src
â”‚       â”œâ”€â”€ ðŸ“‚main
â”‚       â”‚   â””â”€â”€ ðŸ“‚java
â”‚       â”‚       â””â”€â”€ ðŸ“œTestReproduction.java
â”‚       â””â”€â”€ ðŸ“‚test
â”‚           â”œâ”€â”€ ðŸ“‚java
â”‚           â”‚   â””â”€â”€ ðŸ“œTestReproductionTest.java
â”‚           â””â”€â”€ ðŸ“‚resources
â”‚               â”œâ”€â”€ ðŸ“‚data
â”‚               â”‚   â”œâ”€â”€ ðŸ“œbezdekIris.data
â”‚               â”‚   â””â”€â”€ ðŸ“œwinequality-red.csv
â”‚               â””â”€â”€ ðŸ“‚models
â”‚                   â”œâ”€â”€ ðŸ“‚classification
â”‚                   â”‚   â”œâ”€â”€ ðŸ“œ3-nn.model
â”‚                   â”‚   ...
â”‚                   â”‚   â””â”€â”€ ðŸ“œrf.model
â”‚                   â””â”€â”€ ðŸ“‚regression
â”‚                       â”œâ”€â”€ ðŸ“œ3-nn.model
â”‚                       ...
â”‚                       â””â”€â”€ ðŸ“œrf-reg.model
â”œâ”€â”€ ðŸ“‚tribuo-env
â”‚   â””â”€â”€ ðŸ“œDockerfile # Builds an environment tribuo can run in, lengthy build time as it install many dependencies
â”œâ”€â”€ ðŸ“œDockerfile # Copies this repo into the tribuo-env container, occurs quickly as it uses the prebuilt image.
â”œâ”€â”€ ðŸ“œLICENSE
â””â”€â”€ ðŸ“œREADME.md

```
